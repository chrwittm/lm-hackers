{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b7c7e1d-ad0c-4668-a10d-569ba6b7aa04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Running Llama2 on your Mac with `llama.cpp`/`llama-cpp-python`\n",
    "\n",
    "In my [previous notebook](https://github.com/chrwittm/lm-hackers/blob/main/20-local-llama-on-mac/10-running-llama2-on-mac1-hf.ipynb), I ran a llama2 model on Hugging Face which turned out to be quite slow. In this notebook, let's explore [llama.cpp](https://github.com/ggerganov/llama.cpp).\n",
    "\n",
    "The goal of this notebook is to run a local Llama2 model and implement a simple chat functionality to have a conversation with Llama2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92caa1f",
   "metadata": {},
   "source": [
    "## What is `llama.cpp`?\n",
    "\n",
    "[`Llama.cpp`](https://github.com/ggerganov/llama.cpp) is an optimized library to run a Large Language Model (LLM) like Llama2 on a Mac, but it also supports other platforms. How is this possible. For the details, please let me refer to this [tweet by Andrej Karpathy](https://twitter.com/karpathy/status/1691844860599492721) and for even more details to this [blog post by Finbarr Timbers](https://finbarr.ca/how-is-llama-cpp-possible/). Here are my takeaways:\n",
    "\n",
    "- [`Llama.cpp`](https://github.com/ggerganov/llama.cpp) runs inference of LLMs in pure C/C++, therefore, it is significantly faster than implementations in higher languages like python.\n",
    "- Additionally, [the mission](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#description) of the project  _\"is to run the LLaMA model using 4-bit integer quantization on a MacBook\"_. This means that numbers used to represent model weights and activations downsized from 32- or 16- bit floating points (the format of the base models) with 4-bit integers. This reduces memory usage and improves the performance and efficiency of the model during inference. The somewhat surprising thing is that model performance does not degrade by this downsizing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f1b899",
   "metadata": {},
   "source": [
    "## How You Can Use llama.cpp from Python\n",
    "\n",
    "The project [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python) serves as a binding for [`llama.cpp`](https://github.com/ggerganov/llama.cpp), providing access to the C++ API to Llama2 from Python.\n",
    "\n",
    "In this context, a \"[binding](https://en.wikipedia.org/wiki/Language_binding)\" is a bridge that facilitates interaction between two programming languages, i.e. a layer of code that allows two programming languages to interact with each other. [`Llama.cpp`](https://github.com/ggerganov/llama.cpp) is written in C/C++, and the [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python) binding allows this C/C++ library to be utilized within a Python environment. Essentially, the Python code wraps around the C/C++ code so that it can be called from a Python environment.\n",
    "\n",
    "To gain more insight into how the binding works and how C/C++ is called from Python, check out [llama_cpp/llama_cpp.py](https://github.com/abetlen/llama-cpp-python/blob/main/llama_cpp/llama_cpp.py) which uses the Python library `ctypes` that provides C compatible data types and allows calling functions in DLLs or shared libraries. Look for the following: \n",
    "\n",
    "- **Data type conversion**: Data types from the `ctypes` library are imported and used to represent Python variables in a C/C++-compatible way.  \n",
    "- **Loading shared libraries**: The method `ctypes.CDLL` is used to load shared libraries which contain the C/C++ code.\n",
    "- **Direct calls to C/C++**: Methods from shared library which has been loaded are called.\n",
    "\n",
    "In the following code snippet, the parameter value `LLAMA_MAX_DEVICES` is retrieved:\n",
    "\n",
    "```python\n",
    "_lib.llama_max_devices.argtypes = []\n",
    "_lib.llama_max_devices.restype = ctypes.c_size_t\n",
    "\n",
    "LLAMA_MAX_DEVICES = _lib.llama_max_devices()\n",
    "```\n",
    "\n",
    "For a full example of how a C/C++ binding works, check out my separate notebook (TOO BE WRITTEN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea0714d",
   "metadata": {},
   "source": [
    "## Installing `llama-cpp-python`\n",
    "\n",
    "First, we need to [install](https://llama-cpp-python.readthedocs.io/en/latest/#installation) `llama-cpp-python` via `pip install llama-cpp-python`.\n",
    "\n",
    "[Upgrading](https://llama-cpp-python.readthedocs.io/en/latest/#upgrading-and-reinstalling) is done via `pip install llama-cpp-python  --upgrade --force-reinstall --no-cache-dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251de76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-cpp-python\n",
    "#!pip install llama-cpp-python  --upgrade --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a55938",
   "metadata": {},
   "source": [
    "## Downloading the model\n",
    "\n",
    "In this notebook, I am using the following model: [TheBloke/Llama-2-7b-Chat-GGUF](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF)\n",
    "\n",
    "To download the model, please execute the cell below, assuming that you have stored your Hugging Face access token in the `.env`-file. For additional insights/troubleshooting, please also check out [my previous notebook](https://github.com/chrwittm/lm-hackers/blob/main/20-local-llama-on-mac/10-running-llm-on-mac1-hf.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2e7e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/chrwittm/.cache/huggingface/token\n",
      "Login successful\n",
      "--2024-01-30 07:54:55--  https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\n",
      "Resolving huggingface.co (huggingface.co)... 2600:9000:2240:fa00:17:b174:6d00:93a1, 2600:9000:2240:c600:17:b174:6d00:93a1, 2600:9000:2240:7800:17:b174:6d00:93a1, ...\n",
      "Connecting to huggingface.co (huggingface.co)|2600:9000:2240:fa00:17:b174:6d00:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/b0/ca/b0cae82fd4b3a362cab01d17953c45edac67d1c2dfb9fbb9e69c80c32dc2012e/08a5566d61d7cb6b420c3e4387a39e0078e1f2fe5f055f3a03887385304d4bfa?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.Q4_K_M.gguf%3B+filename%3D%22llama-2-7b-chat.Q4_K_M.gguf%22%3B&Expires=1706856895&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjg1Njg5NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iMC9jYS9iMGNhZTgyZmQ0YjNhMzYyY2FiMDFkMTc5NTNjNDVlZGFjNjdkMWMyZGZiOWZiYjllNjljODBjMzJkYzIwMTJlLzA4YTU1NjZkNjFkN2NiNmI0MjBjM2U0Mzg3YTM5ZTAwNzhlMWYyZmU1ZjA1NWYzYTAzODg3Mzg1MzA0ZDRiZmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=UIRwarRNnvJ5Iz56eegfL78EauLJw85qjJRTzc4F9YL4ZG0yFwlEaAcuq%7EBwI4Fv9fXkygD5zyfazr1vZ3ZEPQKhluCSdVJg61w6NU9hjLvzleWmDCEvrreCLeY5reBTnaadaOkT26VBQ4JbMtojvfV-54pKnMfUAQbhiv6vYYfo-zm8XAavZewlb2C0qFF8RVtBEQ%7Ep4XtYjcT7x6sqDEKzxf5diqhn8G9dhazUHnPvGHHaP0d54ukEso74ckNgHvADq1%7EysVFafybsMBOXz2X945pKa%7Eg4zgZ0jcurwmS-efQ8tjpESHpfjOXq1of31ZWN2tW47VCkGHMA1y1llA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2024-01-30 07:54:55--  https://cdn-lfs.huggingface.co/repos/b0/ca/b0cae82fd4b3a362cab01d17953c45edac67d1c2dfb9fbb9e69c80c32dc2012e/08a5566d61d7cb6b420c3e4387a39e0078e1f2fe5f055f3a03887385304d4bfa?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.Q4_K_M.gguf%3B+filename%3D%22llama-2-7b-chat.Q4_K_M.gguf%22%3B&Expires=1706856895&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjg1Njg5NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iMC9jYS9iMGNhZTgyZmQ0YjNhMzYyY2FiMDFkMTc5NTNjNDVlZGFjNjdkMWMyZGZiOWZiYjllNjljODBjMzJkYzIwMTJlLzA4YTU1NjZkNjFkN2NiNmI0MjBjM2U0Mzg3YTM5ZTAwNzhlMWYyZmU1ZjA1NWYzYTAzODg3Mzg1MzA0ZDRiZmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=UIRwarRNnvJ5Iz56eegfL78EauLJw85qjJRTzc4F9YL4ZG0yFwlEaAcuq%7EBwI4Fv9fXkygD5zyfazr1vZ3ZEPQKhluCSdVJg61w6NU9hjLvzleWmDCEvrreCLeY5reBTnaadaOkT26VBQ4JbMtojvfV-54pKnMfUAQbhiv6vYYfo-zm8XAavZewlb2C0qFF8RVtBEQ%7Ep4XtYjcT7x6sqDEKzxf5diqhn8G9dhazUHnPvGHHaP0d54ukEso74ckNgHvADq1%7EysVFafybsMBOXz2X945pKa%7Eg4zgZ0jcurwmS-efQ8tjpESHpfjOXq1of31ZWN2tW47VCkGHMA1y1llA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 2600:9000:225e:d800:11:f807:5180:93a1, 2600:9000:225e:c600:11:f807:5180:93a1, 2600:9000:225e:a000:11:f807:5180:93a1, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|2600:9000:225e:d800:11:f807:5180:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4081004224 (3,8G) [binary/octet-stream]\n",
      "Saving to: ‘../models/llama-2-7b-chat.Q4_K_M.gguf’\n",
      "\n",
      "llama-2-7b-chat.Q4_ 100%[===================>]   3,80G  10,9MB/s    in 6m 4s   \n",
      "\n",
      "2024-01-30 08:01:00 (10,7 MB/s) - ‘../models/llama-2-7b-chat.Q4_K_M.gguf’ saved [4081004224/4081004224]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv('HF_TOKEN')\n",
    "os.environ['HF_TOKEN'] = token\n",
    "!huggingface-cli login --token $HF_TOKEN\n",
    "!wget -P ../models https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f406077",
   "metadata": {},
   "source": [
    "## Loading the Model\n",
    "\n",
    "Loading the model, only required 2 lines of code (see below). Before we execute the cell, let's talk about the parameters:\n",
    "\n",
    "- `n_ctx=2048`: This sets the context window to 2048 tokens. The maximum number of tokens for this model is 4096.\n",
    "- `verbose=False`: This makes the model less talkative. It only prints the actual results when prompted. Please try turning it to `True` to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b824b6c-96cd-4bfe-a615-b3ba4543a92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/Llama-2-7b-chat/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =   156.00 MiB\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"../models/Llama-2-7b-chat/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=2048, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089187e4",
   "metadata": {},
   "source": [
    "## Completion vs. Chat Completion Example\n",
    "\n",
    "There are 2 ways we can talk to the LLM: The Completion-Method literally does what it promises, it completes a prompt. For having a conversation with the LLM, we need to use Chat Completion.\n",
    "\n",
    "As per the [Getting Started guide](https://llama-cpp-python.readthedocs.io/en/latest/#high-level-api), here is one example each on how to use the API:\n",
    "\n",
    "Let's do text completion first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c95a95b9-7c9f-499a-a72a-ad626da8c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9f89cec-5453-4d67-85b2-c937ceb03a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Name the planets in the solar system? A: 1. Earth, 2. Mars, 3. Jupiter, 4. Saturn, 5. Uranus, 6. Ne\n"
     ]
    }
   ],
   "source": [
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8022b27",
   "metadata": {},
   "source": [
    "For the ChatGPT-like experience, we can use the `create_chat_completion` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e84f4acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-4ddbecd0-2511-4c23-9323-f9f961f52636',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1708066246,\n",
       " 'model': '../models/Llama-2-7b-chat/llama-2-7b-chat.Q4_K_M.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \"  Of course! I'd be happy to help you describe the image. Please provide me with the image link or upload the image file, and I will do my best to provide a detailed description of it.\"},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 37, 'completion_tokens': 43, 'total_tokens': 80}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\n",
    "              \"role\": \"system\",\n",
    "              \"content\": \"You are an assistant who perfectly describes images.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Describe this image in detail please.\"\n",
    "          }\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af25ffac",
   "metadata": {},
   "source": [
    "To wrap up this notebook, let's re-write the code to reproduce the [example from the hackers guide](https://github.com/chrwittm/lm-hackers/blob/main/10-open-ai-api/accessing-openai-api.ipynb) to make the LLM talk about money in aussie slang.\n",
    "\n",
    "Since llama.cpp has been created with the goal to have an [OpenAI-compatible interface](https://llama-cpp-python.readthedocs.io/en/latest/), it turns out that is a simple job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d43380c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fair dinkum, mate! Money, eh? It's like the oxygen we breathe, ya know? Can't live without it. (Gets a beer from the fridge) Here, have a cold one while I tell you about this bloody thing called money.\n",
      "Money is like the juice that makes the economic engine go round, mate. It's what we use to buy the things we need and want, like food, shelter, clothes, and a fair dinkum good time. Without it, we'd be as flat as a lizard drinking, ya hear? (Chuckles)\n",
      "But money ain't just for spending, mate. It's also for saving and investing. You gotta put some away for a rainy day, or else you'll be up the creek without a paddle when the bills come due. And don't even get me started on them interest rates, eh? (Winks)\n",
      "Now, I know some blokes might say money is just a piece of paper with dead presidents on it, but that's not true, mate. Money has value because we all agree it does. It's like the magic beans in the old folktale – they're worth something because everyone believes they are. (Nods)\n",
      "But watch out for them scallywags who try to take advantage of us with their fancy financial tricks and schemes, eh? They'll have you believing that a dollar is worth more than it is, mate. Don't fall for it! Keep your wits about you and your hard-earned cash, and you'll be right as rain. (Sips beer)\n",
      "So there you have it, mate – money: the lifeblood of our economy and our daily lives. Without it, we'd be as good as dead in the water. But with it, we can live the dream, eh? (Grins) Now, who's for another cold one?\n"
     ]
    }
   ],
   "source": [
    "aussie_sys = \"You are an Aussie LLM that uses Aussie slang and analogies whenever possible.\"\n",
    "\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": aussie_sys},\n",
    "    {\"role\": \"user\", \"content\": \"What is money?\"}]\n",
    "\n",
    "model_response = llm.create_chat_completion(messages = messages, stream=False)\n",
    "print(model_response['choices'][0]['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
