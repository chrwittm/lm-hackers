{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# default_exp pkg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Chat for Jupyter Notebooks from Scratch\n",
    "\n",
    "Let's build a light-weight chat for llama2 from scratch which can be reused in your Jupyter notebooks.\n",
    "\n",
    "Working exploratively with LLMs, I wanted to not only send prompts to LLMs, but to chat with the LLM from within my Jupyter notebook. It turns out, that building a chat from scratch is not complicated. While I use a [local llama2 model](https://chrwittm.github.io/posts/2024-02-15-running-llama2-on-mac/) in this notebook, the concepts I describe are universal and also transfer to other implementations.\n",
    "\n",
    "Before we get started: This notebook also exists in a [blog post version]() (LINK). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  figure {\n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    text-align: center;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"c-binding-python.png\" alt=\"Dalle: A Python snake winding around a letter C\" style=\"width:50%;\">\n",
    "    <figcaption>Dalle: A Python snake winding around a letter C</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Chat Messages\n",
    "\n",
    "\"Chat Messages\" are the messages you exchange with the LLM. There are 3 roles:\n",
    "\n",
    "- The `system`-message gives overall instructions to the LLM which should be used during the whole chat. It only appears once.\n",
    "- `user`-messages are the messages (\"prompts\") who send to the LLM.\n",
    "- The LLM replies with `assistant`-messages.\n",
    "\n",
    "These messages need to be passed to the LLM via the API in a [JSON format](chat-messages.json). Since the chat is stateless, you need to pass the whole previous conversation to be able to ask follow-up questions. The following animated GIF shows the structure in analogy to ChatGPT and how the messages build-up over time during the chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Chat Messages](chat-messages-thumb.png)](chat-messages.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on that, let's build a class which contains and manages the chat messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ChatMessages:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the Chat.\"\"\"\n",
    "        self._messages = []\n",
    "\n",
    "    def _append_message(self, role, content):\n",
    "        \"\"\"Appends a message with specified role and content to messages list.\"\"\"\n",
    "        self._messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def append_system_message(self, content):\n",
    "        \"\"\"Appends a system message with specified content to messages list.\"\"\"\n",
    "        self._append_message(\"system\", content)\n",
    "\n",
    "    def append_user_message(self, content):\n",
    "        \"\"\"Appends a user message with specified content to messages list.\"\"\"\n",
    "        self._append_message(\"user\", content)\n",
    "\n",
    "    def append_assistant_message(self, content):\n",
    "        \"\"\"Appends an assistant message with specified content to messages list.\"\"\"\n",
    "        self._append_message(\"assistant\", content)\n",
    "    \n",
    "    def get_messages(self):\n",
    "        \"\"\"Returns a shallow copy of the messages list.\"\"\"\n",
    "        return self._messages[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 methods `append_system_message`, `append_user_message`, and `append_assistant_message` call the private method `_append_message` so that there is no confusion as to which message types can be added. The `get_messages` returns a copy of the chat messages, making sure that it is not possible to access the private `_messages` attribute of `ChatMessages`.\n",
    "\n",
    "Let's quickly re-create the example shown on the image above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "chat_messages = ChatMessages()\n",
    "chat_messages.append_system_message(\"For \\\"Question n\\\", please respond with \\\"Response n\\\"\")\n",
    "chat_messages.append_user_message(\"Question 1\")\n",
    "chat_messages.append_assistant_message(\"Response 1\")\n",
    "chat_messages.append_user_message(\"Question 2\")\n",
    "chat_messages.append_assistant_message(\"Response 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"role\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"system\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"content\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"For \\\"Question n\\\", please respond with \\\"Response n\\\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"role\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"user\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"content\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Question 1\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"role\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"assistant\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"content\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Response 1\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"role\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"user\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"content\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Question 2\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"role\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"assistant\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m        \u001b[39;49;00m\u001b[94m\"content\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Response 2\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
      "]\u001b[37m\u001b[39;49;00m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "import json\n",
    "from pygments import highlight\n",
    "from pygments.lexers import JsonLexer\n",
    "from pygments.formatters import TerminalFormatter\n",
    "\n",
    "# Convert messages to a formatted JSON string\n",
    "json_str = json.dumps(chat_messages.get_messages(), indent=4)\n",
    "\n",
    "# Highlight the JSON string to add colors\n",
    "print(highlight(json_str, JsonLexer(), TerminalFormatter()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Chat Version 1\n",
    "\n",
    "We have seen in my [previous blog post](https://chrwittm.github.io/posts/2024-02-15-running-llama2-on-mac/) how we can prompt the llama2 model by calling the `create_chat_completion`-method.\n",
    "\n",
    "The follow class simplifies prompting the LLM by doing the following:\n",
    "\n",
    "- Upon initialization of the chat object, the chat messages are initialized and the system message is added\n",
    "- Whe prompting llama2, both the prompt (the user-message) and the response (the assistant message) are added to the chat messages.\n",
    "- The plain text returned from llama2 is formatted for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from IPython.display import Markdown, clear_output\n",
    "\n",
    "class Llama2ChatVersion2:\n",
    "\n",
    "    def __init__(self, llm, system_message):\n",
    "        \"\"\"Initializes the Chat with the system message.\"\"\"\n",
    "        self._llm = llm\n",
    "        self._chat_messages = ChatMessages()\n",
    "        self._chat_messages.append_system_message(system_message)\n",
    "\n",
    "    def _get_llama2_response(self):\n",
    "        \"\"\"Returns Llama2 model response for given messages.\"\"\"\n",
    "        self.model_response = self._llm.create_chat_completion(self._chat_messages.get_messages())\n",
    "        return self.model_response['choices'][0]['message']['content']\n",
    "\n",
    "    def _format_markdown_with_style(self, text, font_size=16):\n",
    "        \"\"\"Wraps text in a <span> with specified font size, defaults to 16.\"\"\"\n",
    "        return f\"<span style='font-size: {font_size}px;'>{text}</span>\"\n",
    "\n",
    "    def prompt_llama2(self, user_prompt):\n",
    "        \"\"\"Processes user prompt, displays Llama2 response formatted in Markdown.\"\"\"\n",
    "        self._chat_messages.append_user_message(user_prompt)\n",
    "        llama2_response = self._get_llama2_response()\n",
    "        self._chat_messages.append_assistant_message(llama2_response)\n",
    "        display(Markdown(self._format_markdown_with_style(llama2_response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a llama2 instance and prompt it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/Llama-2-7b-chat/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =   156.00 MiB\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"../models/Llama-2-7b-chat/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=2048, verbose=False)\n",
    "#llm = Llama(model_path=\"../../../lm-hackers/models/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=2048, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\n",
       "\n",
       "1. Mercury\n",
       "2. Venus\n",
       "3. Earth\n",
       "4. Mars\n",
       "5. Jupiter\n",
       "6. Saturn\n",
       "7. Uranus\n",
       "8. Neptune</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "chat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\n",
    "chat.prompt_llama2(\"Name the planets in the solar system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks good, but from a useability perspective it is not great, because you have to wait for the whole response to be completed before you see an output. On my machine this takes a few seconds for this short reply. Especially for longer answers, you might, however, start to wonder if the process is running correctly, or is it just the impatient me?\n",
    "\n",
    "In any case, it would be nice to see the model response streamed, i.e. you see how the model writing the answer, the same way you are used to seeing ChatGPT printing out its answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Streaming Works\n",
    "\n",
    "The following cell contains an illustrative example of how streaming works: The function `mock_llm_stream()` returns a generator object because it does not `return` a result, but it `yield`s results. This means that the code is not executed when the function is called, but it only returns a generator object which lazily returns values when it is iterated over. The `for`-loop iterates over the generator object, and each iteration returns a word after some latency, simulating the token generation by the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example for a text streamed via a generator object. "
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "import time\n",
    "\n",
    "def fetch_next_word(words, current_index):\n",
    "    \"\"\"\n",
    "    Mock function to simulate making an API call to fetch the next word from the LLM.\n",
    "    \"\"\"\n",
    "    # Simulate network delay or processing time\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    if current_index < len(words):\n",
    "        return words[current_index]\n",
    "    else:\n",
    "        raise StopIteration(\"End of sentence reached.\")\n",
    "\n",
    "def mock_llm_stream():\n",
    "    sentence = \"This is an example for a text streamed via a generator object.\"\n",
    "    words = sentence.split()\n",
    "    current_index = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Simulate fetching the next word from the LLM\n",
    "            word = fetch_next_word(words, current_index)\n",
    "            yield word\n",
    "            current_index += 1\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "# Capture the generator function in a variable\n",
    "mock_llm_response = mock_llm_stream()\n",
    "\n",
    "# Example of how to use this mock_llm_response\n",
    "for word in mock_llm_response:\n",
    "    print(word, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's stream a response from llama2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\n",
      "\n",
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "4. Mars\n",
      "5. Jupiter\n",
      "6. Saturn\n",
      "7. Uranus\n",
      "8. Neptune"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Answer in a very concise and accurate way\"},\n",
    "    {\"role\": \"user\", \"content\": \"Name the planets in the solar system\"}]\n",
    "\n",
    "model_response = llm.create_chat_completion(messages = messages, stream=True)\n",
    "\n",
    "complete_response = \"\"\n",
    "\n",
    "for part in model_response:\n",
    "    # Check if 'content' key exists in the 'delta' dictionary\n",
    "    if 'content' in part['choices'][0]['delta']:\n",
    "        content = part['choices'][0]['delta']['content']\n",
    "        print(content, end='')\n",
    "        complete_response += content\n",
    "    else:\n",
    "        # Handle the case where 'content' key is not present\n",
    "        # For example, you can print a placeholder or do nothing\n",
    "        # print(\"(no content)\", end='')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Version 2 - Streaming included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include the streaming functionality into our chat messages- and chat-classes. For this we are going to use a nice trick to add additional method from `fastcore`: We can `@patch` the methods into the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "from fastcore.utils import * #for importing patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "@patch    \n",
    "def _get_llama2_response_stream(self:Llama2ChatVersion2):\n",
    "    \"\"\"Returns generator object for streaming Llama2 model responses for given messages.\"\"\"\n",
    "    return self._llm.create_chat_completion(self._chat_messages.get_messages(), stream=True)\n",
    "\n",
    "@patch\n",
    "def prompt_llama2_stream(self:Llama2ChatVersion2, user_prompt):\n",
    "    \"\"\"Processes user prompt in streaming mode, displays updates in Markdown.\"\"\"\n",
    "    self._chat_messages.append_user_message(user_prompt)\n",
    "    llama2_response_stream = self._get_llama2_response_stream()\n",
    "    \n",
    "    complete_stream = \"\"\n",
    "\n",
    "    for part in llama2_response_stream:\n",
    "        # Check if 'content' key exists in the 'delta' dictionary\n",
    "        if 'content' in part['choices'][0]['delta']:\n",
    "            stream_content = part['choices'][0]['delta']['content']\n",
    "            complete_stream += stream_content\n",
    "            #self._chat_messages.append_assistant_stream(stream_content)\n",
    "\n",
    "            # Clear previous output and display new content\n",
    "            clear_output(wait=True)\n",
    "            display(Markdown(self._format_markdown_with_style(complete_stream)))\n",
    "\n",
    "        else:\n",
    "            # Handle the case where 'content' key is not present\n",
    "            pass\n",
    "    \n",
    "    self._chat_messages.append_assistant_message(complete_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the method `prompt_llama2_stream` to get a more interactive response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\n",
       "\n",
       "1. Mercury\n",
       "2. Venus\n",
       "3. Earth\n",
       "4. Mars\n",
       "5. Jupiter\n",
       "6. Saturn\n",
       "7. Uranus\n",
       "8. Neptune</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "chat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\n",
    "chat.prompt_llama2_stream(\"Name the planets in the solar system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for the fun of it, let's continue the chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  Of course! Here are the planets in our solar system listed in reverse order, from farthest to closest to the Sun:\n",
       "1. Neptune\n",
       "2. Uranus\n",
       "3. Saturn\n",
       "4. Jupiter\n",
       "5. Mars\n",
       "6. Earth\n",
       "7. Venus\n",
       "8. Mercury</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "chat.prompt_llama2_stream(\"Please reverse the list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  Sure! Here are the planets in our solar system sorted by their mass, from lowest to highest:\n",
       "1. Mercury (0.38 Earth masses)\n",
       "2. Mars (0.11 Earth masses)\n",
       "3. Venus (0.81 Earth masses)\n",
       "4. Earth (1.00 Earth masses)\n",
       "5. Jupiter (29.6 Earth masses)\n",
       "6. Saturn (95.1 Earth masses)\n",
       "7. Uranus (14.5 Earth masses)\n",
       "8. Neptune (17.1 Earth masses)</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "chat.prompt_llama2_stream(\"Please sort the list by the mass of the planet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping back to the beginning, you can see how the chat is represented in the chat massages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'Answer in a very concise and accurate way'},\n",
       " {'role': 'user', 'content': 'Name the planets in the solar system'},\n",
       " {'role': 'assistant',\n",
       "  'content': '  Sure! Here are the names of the planets in our solar system, listed in order from closest to farthest from the Sun:\\n\\n1. Mercury\\n2. Venus\\n3. Earth\\n4. Mars\\n5. Jupiter\\n6. Saturn\\n7. Uranus\\n8. Neptune'},\n",
       " {'role': 'user', 'content': 'Please reverse the list'},\n",
       " {'role': 'assistant',\n",
       "  'content': '  Of course! Here are the planets in our solar system listed in reverse order, from farthest to closest to the Sun:\\n1. Neptune\\n2. Uranus\\n3. Saturn\\n4. Jupiter\\n5. Mars\\n6. Earth\\n7. Venus\\n8. Mercury'},\n",
       " {'role': 'user', 'content': 'Please sort the list by the mass of the planet'},\n",
       " {'role': 'assistant',\n",
       "  'content': '  Sure! Here are the planets in our solar system sorted by their mass, from lowest to highest:\\n1. Mercury (0.38 Earth masses)\\n2. Mars (0.11 Earth masses)\\n3. Venus (0.81 Earth masses)\\n4. Earth (1.00 Earth masses)\\n5. Jupiter (29.6 Earth masses)\\n6. Saturn (95.1 Earth masses)\\n7. Uranus (14.5 Earth masses)\\n8. Neptune (17.1 Earth masses)'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "chat._chat_messages.get_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this blog post/notebook, we implemented an LLM chat from scratch in a very light-weight format. We learned how the chat messages need to be handled to create the chat experience and we even added streaming support.\n",
    "\n",
    "And the best thing, we can re-use this chat functionality in other notebooks without having to re-write it again, keeping our notebook dry and clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
