{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b7c7e1d-ad0c-4668-a10d-569ba6b7aa04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Running Llama2 on your Mac with Hugging Face\n",
    "\n",
    "Trying to stick as closely as possible to the original [hackers guide by Jeremy Howard](https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb), I tried to run some LLMs locally on my machine. I am working on a MacBook Pro with M2 Max, therefore, I do not have any Nvidia support and I needed to adapt the code to make it compatible with [Apple's Metal Framework](https://developer.apple.com/documentation/metalperformanceshaders).\n",
    "\n",
    "As expected there were some hurdles along the way and performance was not exactly great. Therefore, this is more an experiment, and there are better ways to run LLMs on a Mac which I will explore in another notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fa7911-c191-4945-97aa-6daff95970d7",
   "metadata": {},
   "source": [
    "## Downloading LLama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e15b4d",
   "metadata": {},
   "source": [
    "Before, you can access the Llama2 model, you need to agree to the terms and conditions of Meta for Llama2.\n",
    "\n",
    "As per the time of writing this, the process was as follows:\n",
    "\n",
    "- Visit [the model's home page at Hugging Face](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n",
    "- Go to Meta's website (https://ai.meta.com/resources/models-and-libraries/llama-downloads/), and complete the registration form\n",
    "- Confirm the terms and conditions on the Hugging Face Website (see [screenshot](access-llama2-on-hf.png))\n",
    "\n",
    "The approval only took a couple of minutes.\n",
    "\n",
    "When you want to access the model from a notebook, you need to authenticate with Hugging Face so that they can check that you agreed with the terms and conditions.\n",
    "\n",
    "You can easily create an access token on the [Hugging Face website](https://huggingface.co/settings/tokens) as explained [here](https://huggingface.co/docs/hub/security-tokens).\n",
    "\n",
    "I recommend to a store this token in an environment variable so that you don't have to included into your Jupyter notebook: The environment variable is `HF_TOKEN`, so just add it to your `.env`-file.\n",
    "\n",
    "> ðŸ’¡ Note: For more details on how to manage your access token, please check out my [notebook on accessing the OpenAI API](https://chrwittm.github.io/posts/2024-01-27-how-to-call-openai-api/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ad4b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4da61418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate\n",
    "#!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9256ae9",
   "metadata": {},
   "source": [
    "Once the authorization is solved, you can load the model from Huggingface. The first time you run the code, it will take some time because it needs to download 15 GB of model data.\n",
    "\n",
    "As you can see in the codes, I need to adjust the parameters for compatibility with the Apple Metal Framework:\n",
    "\n",
    "- `device_map=\"auto\"`: The value `0` specifies that a CUDA-compatible device (from Nvidia) should be used. Using the value `auto`, the memory usage [is optimized](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch.device_map) to [first fill all the space in your GPU(s)](https://huggingface.co/docs/accelerate/usage_guides/big_modeling), then into to the CPU, and finally, if there is not enough RAM, it will be loaded to the disk (the absolute slowest option). This also works on Apple Silicon.\n",
    "\n",
    "- `load_in_8bit=False`: Apple silicon currently does not support the same 8-bit quantization optimizations that Nvidia's GPUs do. Therefore, you need to load the model without quantizing it to 8-bit format.\n",
    "\n",
    "> ðŸ’¡ Note: \"Quantization\" refers to the process of reducing the precision of the numbers used to represent model weights and activations. This is typically done to reduce memory usage and improve the performance and efficiency of a model during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e417dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96fbb61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ac71f0760f455a930b11f2b403aef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrwittm/miniforge3/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "token = os.getenv('HF_TOKEN')\n",
    "if token is None:\n",
    "    raise ValueError(\"Hugging Face token not found. Please check your .env file.\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(mn, use_auth_token=token, device_map=0, load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=token , device_map=\"auto\", load_in_8bit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3bd21",
   "metadata": {},
   "source": [
    "There are a few thing to note regarding the download process:\n",
    "\n",
    "- The download of the model is done once, and it is buffered on your disk: The cache directory is: `/Users/<YourUser>/.cache/huggingface/hub`\n",
    "- When loading the model, you can observe that `shards` are loaded. A shard is, in simple terms, a chunk of the model. It is split up into several chunks for efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645dc25",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "The next step is to get the **tokenizer**. A tokenizer is a class, specific to the model which converts words into numbers and vice versa. A token is a numerical representation of a word or a segment of a word.\n",
    "\n",
    "When tokenizing a **prompt**, we request a PyTorch tensor (\"`pt`\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94600cb9-2a64-47e2-aca3-99a72985157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokr = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "prompt = \"Jeremy Howard is a \"\n",
    "toks = tokr(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20605293-b31f-4db7-b6a5-0ef662d89441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  5677,  6764, 17430,   338,   263, 29871]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f2cb6",
   "metadata": {},
   "source": [
    "Converting the numbers back to words returns the same result, preceeded by a special token `<s>` the start token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c20ab287-7d80-47c2-870f-e2ee6bfc4492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> Jeremy Howard is a ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokr.batch_decode(toks['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aeb7b7",
   "metadata": {},
   "source": [
    "## GPU vs. CPU\n",
    "\n",
    "Before we use the model, let's quickly explore the methods `.to(\"mps\")` and `.to(\"cpu\")`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a88e4d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,  5677,  6764, 17430,   338,   263, 29871]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[    1,  5677,  6764, 17430,   338,   263, 29871]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "print(toks.to(\"cpu\"))\n",
    "print(toks.to(\"mps\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32423432",
   "metadata": {},
   "source": [
    "You can spot the difference that the `.to(\"mps\")`-tensor has an additional `device='mps:0'`-element. This means that this tensor is processed on the GPU, unlike the `.to(\"cpu\")`-tensor. In the context of CUDA-acceleration, the analogous statements `.to(\"cuda\")` and `.to(\"cpu\")` would not only switch between GPU and CPU, but it would also copy the tensor from RAM to VRAM (VRAM being the dedicated GPU-Video RAM). The Apple Silicon world operates on Unified Memory (shared memory), meaning that the full memory of a machine can potentially be used by the GPU cores. While this saves time in the Apple Silicon world because there is no copying, the memory of Nvidia cards tends to be faster then the RAM in Apple machines.\n",
    "\n",
    "When we pass the tokens to the LLM, we want the prompt tokens to be on the GPU, because even at inference, the LLM is very compute intensive, and it uses operations like matrix multiplication which can very well be optimized and parallelized on a GPU.\n",
    "\n",
    "We want the resulting tensor to be on the CPU because the subsequent operations (like converting the generated tokens to text) expect a tensor to be on the CPU. In the Apple Silicon world we do not need to free valuable GPU memory, but moving the tensor to the CPU is best for compatibility with further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340a884",
   "metadata": {},
   "source": [
    "## Running Llama2\n",
    "\n",
    "So let's go ahead and generate the response of the model by passing the prompt tokens and requesting new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab3ad5dc-51b4-48ca-92ae-5027069771c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.63 s, sys: 33.6 s, total: 37.3 s\n",
      "Wall time: 3min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  5677,  6764, 17430,   338,   263, 29871, 29896, 29929, 29899,\n",
       "          6360, 29899,  1025,   515,   951,  5779, 29889,   940,   756,  1063,\n",
       "          9701,   297]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "res = model.generate(**toks.to(\"mps\"), max_new_tokens=15).to('cpu')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af5a23fc-aa2a-4ce2-92c7-75364130e4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> Jeremy Howard is a 19-year-old from Leeds. He has been involved in']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokr.batch_decode(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f150fd2a",
   "metadata": {},
   "source": [
    "So far so unperformant:\n",
    "\n",
    "- CPU times: user 3.63 s, sys: 33.6 s, total: 37.3 s\n",
    "- Wall time: 3min 28s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c11e20",
   "metadata": {},
   "source": [
    "The result is pretty poor on my M2 Max with 32GB RAM, especially considering that Jeremy's machine did the same thing in less than 2 seconds. There are probably a couple of reasons which produce this dramatic difference in performance:\n",
    "\n",
    "- Nvidia memory throughput is a lot better then Apple's unified RAM\n",
    "- The model I used was originally optimized and quantized for Nvidia GPUs. To run this model on my MacBook, I had to disable the 8-bit quantization (`load_in_8bit=False`) among other changes. While this adaptation was necessary for compatibility with Apple Silicon, it discarded all the optimizations.\n",
    "- PyTorch's optimization for CUDA is probably still way better than its MPS optimization.\n",
    "\n",
    "Whatever the true reason is, my intuition is that it is not mainly the \"hardware's fault\", but rather the way I ran the model. After all, I had to removed the quantization to get LLama2 to run on my Mac. But there are other options out there, and I will explore them in other notebooks.\n",
    "\n",
    "Spoiler alert: It is possible to run LLama2 on a Mac at a good speed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
