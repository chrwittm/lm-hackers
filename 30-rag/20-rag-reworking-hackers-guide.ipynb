{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Retrieval (RAG) - Reworking Hackers Guide\n",
    "\n",
    "Let's rework the Retrieval Augmented Retrieval (RAG) section from the [hackers guide](https://www.youtube.com/watch?v=jkrNMKz9pWU&t=4232s).\n",
    "\n",
    "This notebook as two main focus points\n",
    "\n",
    " - Re-creating the Jeremy's example on a Mac\n",
    " - Creating my own RAG application with a llama2 instance on my local machine.\n",
    "\n",
    "Back in 2017/2018 my wife and I did a world trip, and we documented it on our blog [Wittmann-Tours.de](https://wittmann-tours.de). These 14 month were among the most exciting times of my life, but nonetheless, I start forgetting details. Therefore, I thought it would be great to have a large language model (LLM) which could help me remember the details.\n",
    "\n",
    "This notebook is also available in a [blog post version](https://chrwittm.github.io/posts/2024-03-22-rag1-remembering-world-trip/) which contains more about the story and background information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "### Loading the Chat \n",
    "\n",
    "As shown in the [chat consumer notebook](https://github.com/chrwittm/lm-hackers/blob/main/20-local-llama-on-mac/35-notebook-chat-consumer.ipynb), let's reuse the chat developed earlier in [this blog post](https://chrwittm.github.io/posts/2024-02-23-chat-from-scratch/) / [this notebook](https://github.com/chrwittm/lm-hackers/blob/main/20-local-llama-on-mac/30-notebook-chat.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../notebook_chat')\n",
    "\n",
    "from notebook_chat import ChatMessages, Llama2ChatVersion2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model\n",
    "\n",
    "Loading the model, only required 2 lines of code (see below). Before we execute the cell, let's talk about the parameters:\n",
    "\n",
    "- `n_ctx=4096`: This sets the context window to 4096 tokens. This is the maximum context window. We will need it as we will discuss later.\n",
    "- `verbose=False`: This makes the model less talkative. It only prints the actual results when prompted. Please try turning it to `True` to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/Llama-2-7b-chat/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =   288.00 MiB\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"../models/Llama-2-7b-chat/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=4096, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading example dataset\n",
    "\n",
    "We are going to work with the [Wittmann-Tours.de](https://wittmann-tours.de) blog.\n",
    "\n",
    "The blog is available for download as a dataset in [the Wittmann-Tours repo](https://github.com/chrwittm/wittmann-tours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -P ../wt-blogposts https://github.com/chrwittm/wittmann-tours/raw/main/zip/blogposts-md.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip -o ../wt-blogposts/blogposts-md.zip -d ../wt-blogposts/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we have all the blog posts in a folder called `wt-blogposts`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting the example from the hacker's guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask the model about Jeremy Howard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  Jeremy Howard is a well-known American actor, musician, and YouTube personality. He was born on July 26, 1984, in San Diego, California, and gained popularity through his YouTube channel, \"Jeremy Howard,\" where he posts vlogs, comedy skits, and other content. He has also appeared in various TV shows and movies, including \"The Goldbergs\" and \"Shameless.\"</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\n",
    "question = \"Who is Jeremy Howard?\"\n",
    "chat.prompt_llama2_stream(f\"{question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is not what we expect in this context. While the answer differs when you run it multiple times, it tends to be something like this: \"Jeremy Howard is a well-known American actor, musician, and YouTube personality...\". Presumeably it referes to [this Jeremy Howard](https://de.wikipedia.org/wiki/Jeremy_Howard).\n",
    "\n",
    "To steer the model in the right direction, let's give it more context to answer the question and pass the [Wikipedia page about \"our\" Jeremy](https://en.wikipedia.org/wiki/Jeremy_Howard_(entrepreneur)) as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wikipedia-api\n",
    "from wikipediaapi import Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeremy Howard (born 13 November 1973) is an Australian data scientist, entrepreneur, and educator.He is the co-founder of fast.ai, where he teaches introductory courses, develops software, and conducts research in the area of deep learning.\n",
      "Previously he founded and led Fastmail, Optimal Decisions Group, and Enlitic. He was President and Chief Scientist of Kaggle.\n",
      "Early in the COVID-19 epidemic he was a leading advocate for masking.\n",
      "\n",
      "Early life\n",
      "Howard was born in London, United Kingdom, and move\n"
     ]
    }
   ],
   "source": [
    "wiki = Wikipedia('JeremyHowardBot/0.0', 'en')\n",
    "jh_page = wiki.page('Jeremy_Howard_(entrepreneur)').text\n",
    "jh_page = jh_page.split('\\nReferences\\n')[0]\n",
    "print(jh_page[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "613"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jh_page.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-phrase the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question with the help of the provided context.\n",
      "\n",
      "## Question\n",
      "\n",
      "Who is Jeremy Howard?\n",
      "\n",
      "## Context\n",
      "\n",
      "Jeremy Howard (born 13 November 1973) is an Australian data scientist, entrepreneur, and educator.He is the co-founder of fast.ai, where he teaches introductory courses, develops software, and conducts research in the area of deep learning.\n",
      "Previously he founded and led Fastmail, Optimal Decisions Group, and Enlitic. He was President and Chief Scientist of Kaggle.\n",
      "Early in the COVID-19 epi\n"
     ]
    }
   ],
   "source": [
    "question_with_context = f\"\"\"Answer the question with the help of the provided context.\n",
    "\n",
    "## Question\n",
    "\n",
    "{question}\n",
    "\n",
    "## Context\n",
    "\n",
    "{jh_page}\"\"\"\n",
    "\n",
    "print(question_with_context[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  Jeremy Howard is an Australian data scientist, entrepreneur, and educator who has made significant contributions to the field of deep learning and machine learning. He is the co-founder of fast.ai, where he teaches introductory courses, develops software, and conducts research in the area of deep learning. Howard has also founded several successful startups, including FastMail and Optimal Decisions Group, and has advised various organizations on data strategy and investment opportunities.\n",
       "Early Life and Education:\n",
       "Jeremy Howard was born in London, UK, and moved to Melbourne, Australia, in 1976. He attended Melbourne Grammar and studied philosophy at the University of Melbourne.\n",
       "Career:\n",
       "Howard started his career in management consulting, working at McKinsey & Co and AT Kearney. He then transitioned into entrepreneurship, founding two successful startups: FastMail and Optimal Decisions Group. He later became involved with Kaggle, where he served as President and Chief Scientist, and helped develop the Perl programming language.\n",
       "Enlitic:\n",
       "In 2014, Howard founded Enlitic to use machine learning to make medical diagnostics and clinical decision support tools faster, more accurate, and more accessible. He believes that machine learning algorithms are as good as or better than humans at many things and has taught data science at Singularity University.\n",
       "fast.ai:\n",
       "Together with Rachel Thomas, Howard co-founded fast.ai, a research institute dedicated to making deep learning more accessible. He teaches introductory courses, both online and in-person, and has developed the ULMFiT algorithm, which pioneered transfer learning and fine-tuning techniques in natural language processing.\n",
       "Personal Life and Interests:\n",
       "Howard is an angel investor and mentors and advises many startups. He has also contributed to a range of open-source projects as a developer and was a regular guest expert on Australia's most popular TV morning news program Sunrise. Howard used Spaced Repetitive Learning to develop usable Chinese language skills in just one year.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\n",
    "chat.prompt_llama2_stream(f\"{question_with_context}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to pick a context\n",
    "\n",
    "With so much information out there, it is challenging to provide the right context for a model. Let's explore how we can use embedding for this task. For more explanations, please check out my [blog post version](https://chrwittm.github.io/posts/2024-03-22-rag1-remembering-world-trip/) of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=0)\n",
    "emb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeremy Howard (born 13 November 1973) is an Australian data scientist, entrepreneur, and educator.He is the co-founder of fast.ai, where he teaches introductory courses, develops software, and conducts research in the area of deep learning.\n",
      "Previously he founded and led Fastmail, Optimal Decisions Group, and Enlitic. He was President and Chief Scientist of Kaggle.\n",
      "Early in the COVID-19 epidemic he was a leading advocate for masking.\n"
     ]
    }
   ],
   "source": [
    "jh = jh_page.split('\\n\\n')[0]\n",
    "print(jh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_page = wiki.page('Tony_Blair').text.split('\\nReferences\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sir Anthony Charles Lynton Blair  (born 6 May 1953) is a British politician who served as Prime Minister of the United Kingdom from 1997 to 2007 and Leader of the Labour Party from 1994 to 2007. He served as Leader of the Opposition from 1994 to 1997 and held various shadow cabinet posts from 1987 to 1994. Blair was Member of Parliament (MP) for Sedgefield from 1983 to 2007. He\n"
     ]
    }
   ],
   "source": [
    "tb = tb_page.split('\\n\\n')[0]\n",
    "print(tb[:380])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling encode, the model returns a tensor of activations for each document, if the activations are close to each other, the documents are similar to each other, if not, the documents contain different content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_emb,jh_emb,tb_emb = emb_model.encode([question,jh,tb], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7991, device='mps:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(q_emb, jh_emb, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5381, device='mps:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(q_emb, tb_emb, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Wittmann-Tours RAG-based LLM\n",
    "\n",
    "Let's do the same thing for Wittmann Tours.\n",
    "\n",
    "Here is a question for our model, it cannot answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  The name of the guide who led your tour in the Masoala rainforest on Madagascar is... (drumroll) ...Rahel!</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#question = \"Wie hieß der Guide, der uns durch den Masoala Regenwald geführt hat?\"\n",
    "question = \"What was the name of the guide who led us on our tour in the Masoala rain forest on Madagascar?\"\n",
    "chat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\n",
    "chat.prompt_llama2_stream(f\"{question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either it hallucinates, or it admits that it cannot figure it out. Let's provide more context.\n",
    "\n",
    "Above, we have downloaded the whole blog [Wittmann Tour.de](https://wittmann-tours.de/) so that every blogpost is a markdown document.\n",
    "\n",
    "Here is the blog post about Masoala:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_blogpost = \"../wt-blogposts/drei-tage-im-masoalaregenwald/index.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The blogpost has 18435 characters\n",
      "---\n",
      "title: 'Drei Tage im Masoala-Regenwald'\n",
      "description: \"\"\n",
      "published: 2019-07-14\n",
      "redirect_from: \n",
      "            - https://wittmann-tours.de/drei-tage-im-masoala-regenwald/\n",
      "categories: \"Brookesia, Chamäleon, Lemur, Madagaskar, Madagaskar, Maki, Masoala, Regenwald, roter Vari, Taggecko, Umweltschutz, Vari, Wald, Wanderung\"\n",
      "hero: ./img/wp-content-uploads-2019-06-CW-20180820-105656-0464-1024x683.jpg\n",
      "---\n",
      "# Drei Tage im Masoala-Regenwald\n",
      "\n",
      "Nach einer knapp 2-stündigen Bootsfahrt von [Nosy Mangabe](http://wittmann-tours.de/nosy-mangabe) aus erreichten wir unser Ziel, die Masoala Forest Lodge. Wir landeten an einem Strand und gingen kaum 200 Meter landeinwärts, wo ein paar hübsche kleine Bungalows auf uns warteten. So viel Luxus hatten wir nach der vorherigen Campingnacht kaum erwartet. Um das gute Wetter - sprich kein Regen - auszunutzen, starteten wir umgehend auf die erste Wanderung durch den Urwald.\n",
      "\n",
      "![Auf der Masoala-Halbinsel befindet sich das größte noch zusammenhängende Regenwaldgebiet Ma\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_blogpost, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(f\"The blogpost has {len(content)} characters\")\n",
    "print(content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_with_context(question, context):\n",
    "    return  f\"\"\"Answer the question with the help of the provided context.\n",
    "\n",
    "    ## Question\n",
    "\n",
    "    {question}\n",
    "\n",
    "    ## Context\n",
    "\n",
    "    {context}\"\"\"\n",
    "\n",
    "#get_question_with_context(question, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can ask the question in context it is important to realize that [the model we use](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF) has a maximum context window of 4096 tokens, since the blog post is longer, I only pass the first section. Realizing this limitation, I will not solve this here, because the main goal it to understand how we can provide context at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  Based on the provided context, the name of the guide who led the tour in the Masoala rainforest is Armand.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#question = \"Wie hieß der Guide, der uns durch den Masoala Regenwald geführt hat?\"\n",
    "question = \"What was the name of the guide who led us on our tour in the Masoala rain forest on Madagascar?\"\n",
    "chat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\n",
    "chat.prompt_llama2_stream(f\"{get_question_with_context(question, content[:6000])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is correct!\n",
    "\n",
    "The next challenge is pick the right blog post automatically. Let's start by gathering all blog posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../wt-blogposts/ritt-auf-paso-peruanos-im-colca-tal/index.md',\n",
       " '../wt-blogposts/tropical-treeclimbing-regenwald-auf-allen-etagen/index.md',\n",
       " '../wt-blogposts/hochland-kulinarisch-coca/index.md']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def get_blog_post_files(path):\n",
    "    # Create a pattern to match all .md files in the directories under the base path\n",
    "    pattern = os.path.join(path_to_blog, \"**/*.md\")\n",
    "\n",
    "    # Use glob to find all files matching the pattern\n",
    "    # The '**' pattern means \"this directory and all subdirectories, recursively\"\n",
    "    # The '*.md' pattern means \"all files ending with .md\"\n",
    "    file_list = glob.glob(pattern, recursive=True)\n",
    "\n",
    "    # file_list now contains the full paths of all .md files\n",
    "    return file_list\n",
    "\n",
    "path_to_blog = \"../wt-blogposts\"\n",
    "files = get_blog_post_files(path_to_blog)\n",
    "files[0:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first blog post in the list deals with horseback riding in Peru:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: 'Ritt auf Paso Peruanos im Colca Tal'\n",
      "description: \"\"\n",
      "published: 2018-11-05\n",
      "redirect_from: \n",
      "            - https://wittmann-tours.de/ritt-auf-paso-peruanos-im-colca-tal/\n",
      "categories: \"Colca, Colca Canyon, Colca Tal, Inka, Paso, Paso Peruano, Peru, Peru, Pferde, Reiten, Viscacha\"\n",
      "hero: ./img/wp-content-uploads-2018-10-CW-20180514-083627-2590-1024x683.jpg\n",
      "---\n",
      "# Ritt auf Paso Peruanos im Colca Tal\n",
      "\n",
      "In Peru ist man zu Recht sehr stolz auf die nationale Pferderasse des Landes, die [Paso Peruanos](https://de.wikipedia.org/wiki/Paso_Peruano). Ihre Besonderheit ist, dass sie eine spezielle, überaus bequeme Gangart haben, den [Paso Llano](https://de.wikipedia.org/wiki/Paso_Peruano#Gangmechanik), ähnlich dem Tölt der Islandpferde. Das Zuchtziel (\"[Brio](https://de.wikipedia.org/wiki/Paso_Peruano#Interieur)\") wird folgendermaßen definiert: \"Eifrige Bereitwilligkeit kombiniert mit energischem Einsatz und ausdrucksvoller Präsentation\". Auf diesen Prachtpferden wollten wir gerne reiten und \n"
     ]
    }
   ],
   "source": [
    "def get_blog_post(path):\n",
    "    with open(path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "print(get_blog_post(files[0])[:1000])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to generating the embeddings for the blog posts, we need to talk about language. The embedding model ([BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)) from the [hacker's guide](https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb) was trained on English, but the blog was written in German. It is surprising that it nonetheless performs well in German, but it would be \"more correct\" to use the multi-lingual model [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3). I have put both models in the next cell for testing. The small english model is a lot faster and it gets the job done, that is why this is the active version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=\"mps\")\n",
    "#emb_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to generating the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(text):\n",
    "    return emb_model.encode(text, convert_to_tensor=True)\n",
    "\n",
    "def get_blog_post_embedding(path):\n",
    "    blog_post_text = get_blog_post(path)\n",
    "    return get_text_embedding(blog_post_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0235, 0.0092, 0.0103, 0.0846, 0.0199], device='mps:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_blog_post_embedding(files[0])[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = get_text_embedding(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0154,  0.0802,  0.0510, -0.0560,  0.0006], device='mps:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embedding[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(embedding1, embedding2):\n",
    "    return F.cosine_similarity(embedding1, embedding2, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first test, let's compare the similarity of the question to the 2 blog posts we saw above, the one about Masoala and the one about Peru:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was the name of the guide who led us on our tour in the Masoala rain forest on Madagascar?\n",
      "../wt-blogposts/ritt-auf-paso-peruanos-im-colca-tal/index.md\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4958, device='mps:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(question)\n",
    "print(files[0])\n",
    "get_similarity(question_embedding, get_blog_post_embedding(files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was the name of the guide who led us on our tour in the Masoala rain forest on Madagascar?\n",
      "../wt-blogposts/drei-tage-im-masoalaregenwald/index.md\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6082, device='mps:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(question)\n",
    "print(path_to_blogpost)\n",
    "get_similarity(question_embedding, get_blog_post_embedding(path_to_blogpost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the similarity is higher for the Masoala blog post.\n",
    "\n",
    "Just for fun, here is another one, but it gets a lower rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3877, device='mps:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity(question_embedding, get_blog_post_embedding(\"../wt-blogposts/essen-mit-stern-hongkong-kulinarisch/index.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine what we have done so far and determine the blog post which fits best for a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_post_as_context(question):\n",
    "\n",
    "    best_match = \"\"\n",
    "    best_match_embedding = get_text_embedding(best_match)\n",
    "    question_embedding = get_text_embedding(question)\n",
    "    best_similarity = get_similarity(question_embedding, best_match_embedding)\n",
    "    #print(best_similarity)\n",
    "\n",
    "    for file in files:\n",
    "        #print(file)\n",
    "        blog_post_embedding = get_blog_post_embedding(file)\n",
    "        blog_post_similarity = get_similarity(question_embedding, blog_post_embedding)\n",
    "        #print(blog_post_similarity)\n",
    "        if blog_post_similarity > best_similarity:\n",
    "            best_similarity = blog_post_similarity\n",
    "            best_match = file\n",
    "    \n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../wt-blogposts/drei-tage-im-masoalaregenwald/index.md'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_blog_post_as_context(\"What was the name of the guide who led us on our tour in the Masoala rain forest on Madagascar?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../wt-blogposts/tansania-kulinarisch-ugali/index.md'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_blog_post_as_context(\"Which culinary specialties did we eat in Tansania?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the final step: Let's ask a question and prompt the model with the context we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  Based on the provided text, the following are the culinary specialties that were eaten in Hong Kong:\n",
       "1. Milktea (Pantyhose Tea) - a traditional Chinese tea served with dim sum at a small, unassuming stall in a bustling market area.\n",
       "2. Afternoon Tea - a classic British tradition served at the Peninsula Hotel, featuring sandwiches, scones, and pastries.\n",
       "3. Gänsebraten (Roast Goose) - a typical Cantonese dish served at a small, unpretentious restaurant in a busy street.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context used: ../wt-blogposts/essen-mit-stern-hongkong-kulinarisch/index.md\n"
     ]
    }
   ],
   "source": [
    "question = \"Which culinary specialties did we eat in Hong Kong?\"\n",
    "blog_post_path = get_blog_post_as_context(question)\n",
    "blog_post_context = get_blog_post(blog_post_path)\n",
    "chat = Llama2ChatVersion2(llm, \"Answer in a very concise and accurate way\")\n",
    "chat.prompt_llama2_stream(f\"{get_question_with_context(question, blog_post_context[:6000])}\")\n",
    "print(f\"Context used: {blog_post_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is correct!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
