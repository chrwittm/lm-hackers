{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "\n",
    "Instead of doing RAG, in this notebook, let's actually re-train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Model\n",
    "\n",
    "If you have not done so, please download the model via the follwoing cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv('HF_TOKEN')\n",
    "os.environ['HF_TOKEN'] = token\n",
    "!huggingface-cli login --token $HF_TOKEN\n",
    "!wget -P ..models/Llama-2-7b-chat/ https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model task before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =   156.00 MiB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = Llama(model_path=\"../models/llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=2048, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display, Markdown\n",
    "\n",
    "class Llama2_chat:\n",
    "\n",
    "    def __init__(self, system_message):\n",
    "        \"\"\"Initializes the Chat with the system message.\"\"\"\n",
    "        self._messages = []\n",
    "        self._messages = self.append_system_message(self._messages, content=system_message)\n",
    "\n",
    "    def append_message(self, messages, role, content):\n",
    "        messages.append({\"role\": role, \"content\": content})\n",
    "        return messages\n",
    "\n",
    "    def append_system_message(self, messages, content):\n",
    "        self.append_message(messages, \"system\", content)\n",
    "        return messages\n",
    "\n",
    "    def append_user_message(self, messages, content):\n",
    "        self.append_message(messages, \"user\", content)\n",
    "        return messages\n",
    "\n",
    "    def append_assistant_message(self, messages, content):\n",
    "        self.append_message(messages, \"assistant\", content)\n",
    "        return messages\n",
    "\n",
    "    def append_assistant_stream(self, messages, content):\n",
    "        if self._messages[-1]['role'] != \"assistant\":\n",
    "            self.append_message(messages, \"assistant\", content)\n",
    "        else:\n",
    "            self._messages[-1]['content'] += content\n",
    "        return messages\n",
    "\n",
    "    def get_llama2_response(self, messages, **kwargs):\n",
    "        self.model_response = llm.create_chat_completion(messages = messages, **kwargs)\n",
    "        return self.model_response['choices'][0]['message']['content']\n",
    "\n",
    "    def get_llama2_response_stream(self, messages, **kwargs):\n",
    "        self.model_response = llm.create_chat_completion(messages = messages, stream=True, **kwargs)\n",
    "        return self.model_response\n",
    "\n",
    "    def format_markdown_with_style(self, text, font_size=16):\n",
    "        \"\"\"\n",
    "        Wraps the given text in a <span> tag with the specified font size.\n",
    "\n",
    "        Parameters:\n",
    "        text (str): The Markdown text to be formatted.\n",
    "        font_size (int, optional): The font size to apply. Defaults to 16.\n",
    "\n",
    "        Returns:\n",
    "        str: The formatted Markdown string with HTML styling.\n",
    "        \"\"\"\n",
    "        return f\"<span style='font-size: {font_size}px;'>{text}</span>\"\n",
    "\n",
    "    def prompt_llama2(self, user_prompt):\n",
    "        self._messages = self.append_user_message(self._messages, content=user_prompt)\n",
    "        llama2_response = self.get_llama2_response(self._messages)\n",
    "        self._messages = self.append_assistant_message(self._messages, content=llama2_response)\n",
    "        display(Markdown(self.format_markdown_with_style(llama2_response)))\n",
    "\n",
    "    def prompt_llama2_stream(self, user_prompt):\n",
    "        self._messages = self.append_user_message(self._messages, content=user_prompt)\n",
    "        llama2_response_stream = self.get_llama2_response_stream(self._messages)\n",
    "        \n",
    "        complete_stream = \"\"\n",
    "        display_id = 'llama2_stream_display'  # Unique display ID\n",
    "\n",
    "        for part in llama2_response_stream:\n",
    "            # Check if 'content' key exists in the 'delta' dictionary\n",
    "            if 'content' in part['choices'][0]['delta']:\n",
    "                stream_content = part['choices'][0]['delta']['content']\n",
    "                complete_stream += stream_content\n",
    "                self._messages = self.append_assistant_stream(self._messages, content=stream_content)\n",
    "\n",
    "                # Clear previous output and display new content\n",
    "                clear_output(wait=True)\n",
    "                display(Markdown(self.format_markdown_with_style(complete_stream)))\n",
    "\n",
    "            else:\n",
    "                # Handle the case where 'content' key is not present\n",
    "                # For example, you can print a placeholder or do nothing\n",
    "                # print(\"(no content)\", end='')\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Llama2_chat(\"Answer in a very concise and accurate way\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  I'm just an AI and do not have access to specific information about Wittmann-Tours.de unless it is publicly available on the internet. However, I can suggest some possible ways to find out more about the website:\n",
       "1. Check the website's domain registration information: You can use a whois lookup tool to see when the domain was registered, who the registrant is, and other details. This information may not be up-to-date or accurate, but it can give you an idea of when the website was created and who is responsible for it.\n",
       "2. Look for reviews or testimonials: Check review websites like TripAdvisor, Google Reviews, or Yelp to see what other customers have to say about Wittmann-Tours.de. Be cautious of fake reviews, and look for patterns in the feedback.\n",
       "3. Check the website's social media accounts: Look for the website's presence on social media platforms like Facebook, Twitter, or Instagram. This can give you an idea of how active the website is and what kind of content they post.\n",
       "4. Search for news articles or press releases: You can use a search engine to look for news articles or press releases about Wittmann-Tours.de. This may provide information about the company's history, mission, and any notable events or achievements.\n",
       "5. Contact the website directly: If you have specific questions about Wittmann-Tours.de, you can try contacting them directly through their website or by phone. They may be able to provide you with more detailed information about their services and policies.\n",
       "It's important to note that some websites may not have a lot of publicly available information, especially if they are small or new. In these cases, it may be best to wait and see if more information becomes available over time.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#chat.prompt_llama2(\"Name the planets in the solar system\")\n",
    "#chat.prompt_llama2_stream(\"Please describe the route of the world trip for Wittmann Tours\")\n",
    "#chat.prompt_llama2_stream(\"What do you know about the Wittmann Tours world trip and its itinerary?\")\n",
    "#chat = Llama2_chat(\"Answering the context of the blog wittmann-tours.de. Be truthful, do not make things up. If you no not know, just be honest about it.\")\n",
    "#chat.prompt_llama2_stream(\"What do you know about the Wittmann Tours world trip and its itinerary?\")\n",
    "chat.prompt_llama2_stream(\"What do you know about wittmann-tours.de?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model obviously does not know anything about wittmann-tours.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "The goal is just to show the model additional data. Therefore, I follow the approach as layed out in [this tutorial](https://blog.gopenai.com/how-to-fine-tune-llama-2-on-mac-studio-4b42f317c975):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\ntitle: \\'Ritt auf Paso Peruanos im Colca Tal\\'\\ndescription: \"\"\\npublished: 2018-11-05\\nredirect_from: \\n            - https://wittmann-tours.de/ritt-auf-paso-peruanos-im-colca-tal/\\ncategories: \"Colca, Colca Canyon, Colca Tal, Inka, Paso, Paso Peruano, Peru, Peru, Pferde, Reiten, Viscacha\"\\nhero: ../../../defaultHero.jpg\\n---\\nIn Peru ist man zu Recht sehr stolz auf die nationale Pferderasse des Landes, die [Paso Peruanos](https://de.wikipedia.org/wiki/Paso_Peruano). Ihre Besonderheit ist, dass sie eine spezielle, √ºberaus bequeme Gangart haben, den [Paso Llano](https://de.wikipedia.org/wiki/Paso_Peruano#Gangmechanik), √§hnlich dem T√∂lt der Islandpferde. Das Zuchtziel (\"[Brio](https://de.wikipedia.org/wiki/Paso_Peruano#Interieur)\") wird folgenderma√üen definiert: \"Eifrige Bereitwilligkeit kombiniert mit energischem Einsatz und ausdrucksvoller Pr√§sentation\". Auf diesen Prachtpferden wollten wir gerne reiten und zwar in der herrlichen Landschaft des Colca-Tales.\\n\\n![Colca-Panorama mit unseren Pfe'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "path_to_blog = \"/Users/chrwittm/Library/CloudStorage/OneDrive-Personal/Dokumente/GitRepos/clones/wordpress-to-markdown/out\"\n",
    "\n",
    "def get_blog_post_files(path):\n",
    "    # Create a pattern to match all .mdx files in the directories under the base path\n",
    "    pattern = os.path.join(path_to_blog, \"**/*.mdx\")\n",
    "\n",
    "    # Use glob to find all files matching the pattern\n",
    "    # The '**' pattern means \"this directory and all subdirectories, recursively\"\n",
    "    # The '*.mdx' pattern means \"all files ending with .mdx\"\n",
    "    file_list = glob.glob(pattern, recursive=True)\n",
    "\n",
    "    # file_list now contains the full paths of all .mdx files\n",
    "    return file_list\n",
    "\n",
    "def get_blog_post(path):\n",
    "    # Open the file in read mode\n",
    "    with open(path, 'r') as file:\n",
    "        # Read the contents of the file\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "\n",
    "files = get_blog_post_files(path_to_blog)\n",
    "get_blog_post(files[0])[:1000]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>---\\ntitle: \\'Ritt auf Paso Peruanos im Colca Tal\\'\\ndescription: \"\"\\npublished: 2018-11-05\\nredirect_from: \\n            - https://wittmann-tours.de/ritt-auf-paso-peruanos-im-colca-tal/\\ncategories: \"Colca, Colca Canyon, Colca Tal, Inka, Paso, Paso Peruano, Peru, Peru, Pferde, Reiten, Viscacha\"\\nhero: ../../../defaultHero.jpg\\n---\\nIn Peru ist man zu Recht sehr stolz auf die nationale Pferderasse des Landes, die [Paso Peruanos](https://de.wikipedia.org/wiki/Paso_Peruano). Ihre Besonderheit ist, dass sie eine spezielle, √ºberaus bequeme Gangart haben, den [Paso Llano](https://de.wikipedia.org/wiki/Paso_Peruano#Gangmechanik), √§hnlich dem T√∂lt der Islandpferde. Das Zuchtziel (\"[Brio](https://de.wikipedia.org/wiki/Paso_Peruano#Interieur)\") wird folgenderma√üen definiert: \"Eifrige Bereitwilligkeit kombiniert mit energischem Einsatz und ausdrucksvoller Pr√§sentation\". Auf diesen Prachtpferden wollten wir gerne reiten und zwar in der herrlichen Landschaft des Colca-Tales.\\n\\n![Colca-Panorama mit unseren Pfe'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_starting_token(content):\n",
    "    # Add the starting token <s> to the beginning of the content\n",
    "    return \"<s>\" + content\n",
    "\n",
    "add_starting_token(get_blog_post(files[0])[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chrwittm/Library/CloudStorage/OneDrive-Personal/Dokumente/GitRepos/lm-hackers\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_blog_posts_raw(base_path, output_file_path):\n",
    "    # Get a list of all blog post files\n",
    "    blog_post_files = get_blog_post_files(base_path)\n",
    "\n",
    "    # Open the output file in write mode\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        # Iterate over each blog post file\n",
    "        for file_path in blog_post_files:\n",
    "            # Get the content of the blog post\n",
    "            content = get_blog_post(file_path)\n",
    "\n",
    "            # Write the modified content to the output file\n",
    "            output_file.write(content + \"\\n\\n\")  # Add a newline for separation between posts\n",
    "\n",
    "# Example usage\n",
    "output_path = \"all-wittmann-tours-blog-posts-raw.txt\"\n",
    "combine_blog_posts_raw(path_to_blog, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_blog_posts(base_path, output_file_path):\n",
    "    # Get a list of all blog post files\n",
    "    blog_post_files = get_blog_post_files(base_path)\n",
    "\n",
    "    # Open the output file in write mode\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        # Iterate over each blog post file\n",
    "        for file_path in blog_post_files:\n",
    "            # Get the content of the blog post\n",
    "            content = get_blog_post(file_path)\n",
    "\n",
    "            # Add the starting token to the content\n",
    "            modified_content = add_starting_token(content)\n",
    "\n",
    "            # Write the modified content to the output file\n",
    "            output_file.write(modified_content + \"\\n\\n\")  # Add a newline for separation between posts\n",
    "\n",
    "# Example usage\n",
    "output_path = \"all-wittmann-tours-blog-posts.txt\"\n",
    "combine_blog_posts(path_to_blog, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out this image: http://wittmann-tours.de/2018/10/CW-20180514-150742.jpg\n"
     ]
    }
   ],
   "source": [
    "def shorten_image_links(content):\n",
    "    # Regular expression to match the specific format of the image URLs\n",
    "    url_pattern = r'http://wittmann-tours\\.de/wp-content/uploads/(\\d{4}/\\d{2}/CW-\\d{8}-\\d{6})-[^.]+\\.jpg'\n",
    "\n",
    "    # Function to construct the shortened URL\n",
    "    def url_replacer(match):\n",
    "        return f\"http://wittmann-tours.de/{match.group(1)}.jpg\"\n",
    "\n",
    "    # Replace all occurrences of the image URLs with the shortened version\n",
    "    modified_content = re.sub(url_pattern, url_replacer, content)\n",
    "    return modified_content\n",
    "\n",
    "# Example usage\n",
    "content = \"Check out this image: http://wittmann-tours.de/wp-content/uploads/2018/10/CW-20180514-150742-0612-HDR-1024x683.jpg\"\n",
    "modified_content = shorten_image_links(content)\n",
    "print(modified_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an image: ![Die Reittour im Colca-Tal beginnt](http://wittmann-tours.de/2018/10/CW-20180514-132856.jpg)\n",
      "<s>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def insert_token_after_image_link(content, token=\"<s>\"):\n",
    "    # Regular expression to match markdown image links\n",
    "    image_link_pattern = r'!\\[.*?\\]\\(.*?\\)'\n",
    "\n",
    "    # Function to add the token after the image link\n",
    "    def add_token(match):\n",
    "        return match.group(0) + '\\n' + token\n",
    "\n",
    "    # Replace the image links with the image link followed by the token\n",
    "    modified_content = re.sub(image_link_pattern, add_token, content)\n",
    "    return modified_content\n",
    "\n",
    "# Example usage\n",
    "content = \"Here is an image: ![Die Reittour im Colca-Tal beginnt](http://wittmann-tours.de/2018/10/CW-20180514-132856.jpg)\"\n",
    "modified_content = insert_token_after_image_link(content)\n",
    "print(modified_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_starting_token_to_blog_post(content):\n",
    "    # Split the content into lines\n",
    "    lines = content.split('\\n')\n",
    "\n",
    "    # Add \"<s>\" at the beginning of the content\n",
    "    lines.insert(0, \"<s>\")\n",
    "\n",
    "    # Iterate through lines and add \"<s>\" before each level 2 heading\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith(\"##\"):\n",
    "            lines[i] = \"<s>\\n\" + line\n",
    "\n",
    "    # Combine the lines back into a single string\n",
    "    modified_content = '\\n'.join(lines)\n",
    "    return modified_content\n",
    "\n",
    "import re\n",
    "\n",
    "#def replace_image_links(content, replacement_token=\"<foto>\\n<s>\"):\n",
    "#    # Regular expression to match typical image URLs\n",
    "#    image_url_pattern = r'\\(http[s]?://[^\\s]*\\.(jpg|jpeg|png|gif)\\)'\n",
    "#    \n",
    "#    # Replace all occurrences of the image URLs with the replacement token\n",
    "#    modified_content = re.sub(image_url_pattern, replacement_token, content)\n",
    "#    return modified_content\n",
    "\n",
    "def replace_image_links(content, replacement_token=\"<foto>\\n<s>\"):\n",
    "    # Regular expression to match typical image URLs and local image paths\n",
    "    image_url_pattern = r'\\(http[s]?://[^\\s]*\\.(jpg|jpeg|png|gif)\\)|\\./img/wp-content-uploads-[^\\s]*\\.(jpg|jpeg|png|gif)'\n",
    "\n",
    "    # Replace all occurrences of the image URLs and local image paths with the replacement token\n",
    "    modified_content = re.sub(image_url_pattern, replacement_token, content)\n",
    "    return modified_content\n",
    "\n",
    "# Example usage\n",
    "# content = \"Here is a remote image (http://example.com/image.jpg) and a local image (./img/wp-content-uploads-2019-03-image.jpg)\"\n",
    "# modified_content = replace_image_links(content)\n",
    "# print(modified_content)\n",
    "\n",
    "def replace_links(content, replacement_token=\"<link>\"):\n",
    "    # Regular expression to match typical URLs\n",
    "    url_pattern = r'\\bhttp[s]?://[^\\s]+\\b'\n",
    "    \n",
    "    # Replace all occurrences of URLs with the replacement token\n",
    "    modified_content = re.sub(url_pattern, replacement_token, content)\n",
    "    return modified_content\n",
    "\n",
    "# Example usage\n",
    "# content = \"Check out this website: http://example.com\"\n",
    "# modified_content = replace_links(content)\n",
    "# print(modified_content)\n",
    "\n",
    "def replace_text(content, text_to_replace, replacement_text):\n",
    "    # Replace all occurrences of text_to_replace with replacement_text\n",
    "    modified_content = content.replace(text_to_replace, replacement_text)\n",
    "    return modified_content\n",
    "\n",
    "# Example usage\n",
    "# content = \"This is a test --- to see if --- works.\"\n",
    "# modified_content = replace_text(content, \"---\", \"<s>\")\n",
    "# print(modified_content)\n",
    "\n",
    "def cleanup_consecutive_start_tokens(content):\n",
    "    # Regular expression to match consecutive <s> tokens separated by whitespace or line feeds\n",
    "    pattern = r'(<s>\\s*)+'\n",
    "\n",
    "    # Replace matched patterns with a single <s> token\n",
    "    cleaned_content = re.sub(pattern, '<s>\\n', content)\n",
    "    return cleaned_content\n",
    "\n",
    "# Example usage\n",
    "# content = \"<s>\\n\\n<s>\\n   <s>\\n<s>\"\n",
    "# cleaned_content = cleanup_consecutive_start_tokens(content)\n",
    "# print(cleaned_content)\n",
    "\n",
    "\n",
    "\n",
    "def combine_blog_posts(file_list, combined_file_path):\n",
    "    with open(combined_file_path, 'w') as combined_file:\n",
    "        for file_path in file_list:\n",
    "            # Get the content of each blog post\n",
    "            content = get_blog_post(file_path)\n",
    "            # Replace photos with token\n",
    "            #content = replace_image_links(content) \n",
    "            # Replace links with token\n",
    "            #content = replace_links(content) \n",
    "            # Replace special cases\n",
    "\n",
    "            #shorten links\n",
    "            content = shorten_image_links(content)\n",
    "            content = insert_token_after_image_link(content)\n",
    "            content = replace_text(content, \"---\", \"<s>\")\n",
    "            content = replace_text(content, \"<!--more-->\", \"<s>\")\n",
    "            content = replace_text(content, \"<s>)\", \"<s>\")\n",
    "            content = replace_text(content, \"![]()\", \"\")\n",
    "            content = replace_text(content, \"hero: ../../../defaultHero.jpg\", \"\")\n",
    "            content = replace_text(content, 'description: \"\"', \"\")\n",
    "            content = replace_text(content, \"title: '\", \"# \")\n",
    "            content = replace_text(content, \"'\\n\", \"\\n\")\n",
    "            content = replace_text(content, \"\\n \\n\", \"\\n\")\n",
    "            \n",
    "\n",
    "            # Add the starting token to the blog post\n",
    "            modified_content = add_starting_token_to_blog_post(content)\n",
    "\n",
    "            modified_content = cleanup_consecutive_start_tokens(modified_content)\n",
    "\n",
    "            # Write the modified content to the combined file\n",
    "            combined_file.write(modified_content + '\\n\\n')\n",
    "\n",
    "# Example usage\n",
    "file_list = get_blog_post_files(path_to_blog)\n",
    "combine_blog_posts(file_list, 'all-wittmann-tours-blog-posts3.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat = Llama2_chat(\"Answering the context of the blog wittmann-tours.de. Be truthful, do not make things up. If you no not know, just be honest about it.\")\n",
    "#chat = Llama2_chat(\"Answer in a very concise and accurate way\")\n",
    "#chat = Llama2_chat(\"Antworte kurz und pr√§gnant\")\n",
    "#chat.prompt_llama2_stream(\"What do you know about the Wittmann Tours world trip and its itinerary?\")\n",
    "#chat.prompt_llama2_stream(\"What do you know about wittmann-tours.de?\")\n",
    "#chat.prompt_llama2_stream(\"Was kannst Du mir √ºber wittmann-tours.de erz√§hlen?\")\n",
    "#chat.prompt_llama2_stream(\"Tell me about fruit juices in Brazil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It failed on the question \"What do you know about the Wittmann Tours world trip and its itinerary?\"- why? Base model?\n",
    "\n",
    "\"The Wittmann Tours World Trip is a once-in-a-lifetime adventure that takes travelers on an epic journey across six continents in just 12 months. Here's a brief overview of the itinerary:\n",
    "\n",
    "Africa: Start the trip with a safari in South Africa, followed by visits to Namibia, Botswana, Zambia, and Kenya.\n",
    "\n",
    "Asia: Travel through India, Nepal, Bhutan, Myanmar, Vietnam, Cambodia, Thailand, Indonesia, and Japan.\n",
    "\n",
    "Australia & Oceania: Explore Australia's iconic landmarks, including Uluru (Ayers Rock), the Great Barrier Reef, and Sydney Harbour Bridge. Visit New Zealand for its stunning fjords, glaciers, and geothermal wonders.\n",
    "\n",
    "South America: Journey through Brazil's Amazon rainforest, visit the Galapagos Islands in Ecuador, explore Peru's Inca Trail and Machu Picchu, and experience Argentina's Patagonian glaciers and Iguazu Falls.\n",
    "\n",
    "Europe: Visit the world-famous cities of Paris, Rome, Barcelona, and London, as well as lesser-known gems like Croatia, Albania, and Bosnia & Herzegovina.\n",
    "\n",
    "North America: Explore the Grand Canyon in Arizona, Yellowstone National Park in Wyoming, and the Rocky Mountains in Canada.\n",
    "\n",
    "The itinerary includes a mix of guided tours, cultural experiences, and free time for independent exploration. The trip also offers opportunities to participate in local activities like hiking, biking, rafting, and wildlife encounters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° **Practical tip** for fine-tuning: On a Mac, make sure your power settings to not reduce the speed when the screen is locked - most likely you do not want to watch the whole process üòâ. On Sonoma, go to `System Settings > Battery > Options...` and activate the setting `Prevent automatic sleeping on power adapter when the display is off`. This way your Mac does not slow down when you are away from the keyboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process of failure\n",
    "\n",
    "### Iteration 1 - Following tutorial of children story\n",
    "\n",
    "Training the same way as in the tutorial used too much RAM, and the context length was too long. Therefore, I did the following\n",
    "\n",
    "- Adjust the training data to reduce the number of tokens to fit it into 512/256\n",
    "- let it run for 18h\n",
    "\n",
    "```bash\n",
    "#./finetune --model-base ./llama-2-7b-chat.Q4_K_M.gguf --train-data all-wittmann-tours-blog-posts1.txt --threads 6 --sample-start \"<s>\" --ctx 4096\n",
    "#./finetune --model-base ./llama-2-7b-chat.Q4_K_M.gguf --train-data all-wittmann-tours-blog-posts1.txt --threads 6 --sample-start \"<s>\" --ctx 512\n",
    "#./finetune --model-base ./llama-2-7b-chat.Q4_K_M.gguf --train-data all-wittmann-tours-blog-posts2.txt --threads 10 --sample-start \"<s>\" --ctx 256 --adam-iter 10 \n",
    "```\n",
    "\n",
    "After all optimizations with reducing tokens, this is the way to go:\n",
    "\n",
    "`./finetune --model-base ./llama-2-7b-chat.Q4_K_M.gguf --train-data all-wittmann-tours-blog-posts2.txt --threads 10 --sample-start \"<s>\" --ctx 512 --adam-iter 319`\n",
    "\n",
    "tokenize_file: warning: found 98 samples (max length 1217) that exceed context length of 512. samples will be cut off.\n",
    "tokenize_file: warning: found 3086 samples (min length 3) that are shorter than context length of 512.\n",
    "tokenize_file: total number of samples: 3184\n",
    "\n",
    "\n",
    "tokenize_file: warning: found 8 samples (max length 749) that exceed context length of 512. samples will be cut off.\n",
    "tokenize_file: warning: found 3104 samples (min length 4) that are shorter than context length of 512.\n",
    "tokenize_file: total number of samples: 3112\n",
    "\n",
    "`./finetune --model-base ./llama-2-7b-chat.Q4_K_M.gguf --train-data all-wittmann-tours-blog-posts2.txt --threads 10 --sample-start \"<s>\" --ctx 512 --adam-iter 312`\n",
    "\n",
    "https://medium.com/@yukiarimo/unlock-the-full-potential-of-your-macbook-with-llama-cpp-run-and-fine-tune-llms-locally-0881f6b9c08d\n",
    "\n",
    "The result was not good, and it kept saying that I need a base model similar to this:\n",
    "\n",
    "```bash\n",
    "./main --model ./models/llama-2-7b/ggml-model-q4_0.gguf\n",
    "       --lora ggml-lora-LATEST-f32.gguf\n",
    "       --lora-base ./models/llama-2-7b/ggml-model-f16.gguf\n",
    "       --prompt \"Can you please write a children's story with 200 words about father and son and friendship and bravery?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the way to create a base model:\n",
    "\n",
    "How to download: https://llama-2.ai/getting-started-with-llama-2/\n",
    "How to convert: https://www.secondstate.io/articles/convert-pytorch-to-gguf/\n",
    "\n",
    "```bash\n",
    "mkdir models\n",
    "cd models\n",
    "wget https://raw.githubusercontent.com/facebookresearch/llama/main/download.sh\n",
    "bash download.sh\n",
    "# - enter link from e-mail\n",
    "# - enter 7B-chat\n",
    "\n",
    "pip install gguf\n",
    "\n",
    "python convert.py models/Llama-2-7b-chat/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  Wittmann Tours is a German tour operator that specializes in organizing tours to various destinations around the world, including Europe, Asia, Africa, and South America. They offer a wide range of travel services such as group tours, private tours, and tailor-made itineraries for individuals or small groups. Their website provides information on their tour packages, destinations, pricing, and contact details.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1st iteration\n",
    "llm = Llama(model_path=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "            lora_path=\"./finetune1/ggml-lora-LATEST-f32.gguf\",\n",
    "            lora_base=\"models/Llama-2-7b-chat/ggml-model-f16.gguf\",\n",
    "            n_ctx=2048, verbose=False)\n",
    "\n",
    "chat = Llama2_chat(\"Answer in a very concise and accurate way\")\n",
    "chat.prompt_llama2_stream(\"What do you know about wittmann-tours.de?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2 - follow the docs\n",
    "\n",
    "In the docs, there is no start token:\n",
    "https://github.com/ggerganov/llama.cpp/blob/master/examples/finetune/README.md\n",
    "\n",
    "```bash\n",
    "./finetune \\\n",
    "        --model-base ./llama-2-7b-chat.Q4_K_M.gguf \\\n",
    "        --lora-out lora-llama-wittmann-tours-ITERATION.bin \\\n",
    "        --train-data \"all-wittmann-tours-blog-posts-raw.txt\" \\\n",
    "        --save-every 100 \\\n",
    "        --threads 10 --adam-iter 2160 --batch 4 --ctx 64 \\\n",
    "        --use-checkpointing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='font-size: 16px;'>  Wittmann-Tours ist ein Reiseblog, der sich auf die Themen \"Reisen\" und \"Kulinarisch\" fokussiert. Er wurde von Christian Wittmann gegr√ºndet und seit 2014 wird er aktuell (Stand: 2020) gepflegt. Der Blog ist international orientiert und berichtet √ºber Reisegeschichten, Tipps, Tricks, Eindr√ºcke und Gerichte aus vielen L√§ndern der Welt. Die meisten Beitr√§ge sind in deutscher Sprache geschrieben, aber auch einige in Englisch.\n",
       "\n",
       "Wittmann-Tours ist ein sehr informativierter Blog, der sich auf die Themen \"Kulinarisch\" fokussiert. Christian Wittmann berichtet √ºber seine eigenen Reiseregeln, Tipps f√ºr den Reiseanfang, die besten Gerichte und Lokalit√§ten, die er gefunden hat, und er gibt auch einige Tipps f√ºr die Reisenden mit sich.\n",
       "\n",
       "Der Blog ist sehr lesenswert und bietet eine gute Grundlage f√ºr Reiseroute-Entscheidungen, da Christian Wittmann viele Erfahrungen gesammelt hat und auf vielen Seiten die besten L√§nder und Orte beschrieben wird. Die Beitr√§ge sind sehr detailliert und informativ, aber auch sehr lesenswert und unterhaltsam.\n",
       "\n",
       "Insgesamt ist Wittmann-Tours ein sehr n√ºtzlicher Blog f√ºr Reisende, die gerne gute Tipps und Eindr√ºcke aus der Welt der Reise bekommen m√∂chten.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2nd iteration\n",
    "llm = Llama(model_path=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "            lora_path=\"./finetune2/lora-llama-wittmann-tours-LATEST.bin\",\n",
    "            lora_base=\"models/Llama-2-7b-chat/ggml-model-f16.gguf\",\n",
    "            n_ctx=2048, verbose=False)\n",
    "\n",
    "chat = Llama2_chat(\"Answer in a very concise and accurate way\")\n",
    "chat.prompt_llama2_stream(\"What do you know about wittmann-tours.de?\")\n",
    "# This iteration seems to produce the best results, but it is not really good. Try the full load on a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the results were a lot better, but I only trained for 16h and it would have taken many days -> do it again on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 3 - optimizing the first approach\n",
    "\n",
    "Since it's ration, two worked better than iteration one, I thought the root cause may be that the first iteration did not have the proper context, for example, it lacked the connection to Wittmann Tours.\n",
    "Therefore, I tried again to train on pretty much the raw data, but this also turned out to be incorrect.\n",
    "\n",
    "```bash\n",
    "./finetune \\\n",
    "        --model-base ./llama-2-7b-chat.Q4_K_M.gguf \\\n",
    "        --lora-out lora-llama-wittmann-tours-ITERATION.bin \\\n",
    "        --train-data \"all-wittmann-tours-blog-posts3.txt\" \\\n",
    "        --save-every 100 \\\n",
    "        --sample-start \"<s>\" \\\n",
    "        --threads 10 --adam-iter 620 --batch 4 --ctx 512 \\\n",
    "        --use-checkpointing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd iteration\n",
    "llm = Llama(model_path=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "            lora_path=\"./lora-llama-wittmann-tours-LATEST.bin\",\n",
    "            lora_base=\"models/Llama-2-7b-chat/ggml-model-f16.gguf\",\n",
    "            n_ctx=2048, verbose=False)\n",
    "\n",
    "chat = Llama2_chat(\"Answer in a very concise and accurate way\")\n",
    "chat.prompt_llama2_stream(\"What do you know about wittmann-tours.de?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 4: Iteration 2 on Paperspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve Login first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
